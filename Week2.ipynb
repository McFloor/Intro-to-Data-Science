{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uc96HGV0pYkJ"
      },
      "source": [
        "# Introduction to Data Science 2025\n",
        "\n",
        "# Week 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWh-qfRJpYkK"
      },
      "source": [
        "## Exercise 1 | Titanic: data preprocessing and imputation\n",
        "<span style=\"font-weight: bold\"> *Note: You can find tutorials for NumPy and Pandas under 'Useful tutorials' in the course material.*</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZy31vJxpYkK"
      },
      "source": [
        "Download the [Titanic dataset](https://www.kaggle.com/c/titanic) [train.csv] from Kaggle or <span style=\"font-weight: 500\">directly from the course material</span>, and complete the following exercises. If you choose to download the dataset from Kaggle, you will need to create a Kaggle account unless you already have one, but it is quite straightforward.\n",
        "\n",
        "The dataset consists of personal information of all the passengers on board the RMS Titanic, along with information about whether they survived the iceberg collision or not.\n",
        "\n",
        "1. Your first task is to read the data file and print the shape of the data.\n",
        "\n",
        "    <span style=\"font-weight: 500\"> *Hint 1: You can read them into a Pandas dataframe if you wish.*</span>\n",
        "    \n",
        "    <span style=\"font-weight: 500\"> *Hint 2: The shape of the data should be (891, 12).*</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4kCNDespYkK"
      },
      "outputs": [],
      "source": [
        "# Use this cell for your code\n",
        "import pandas as pd\n",
        "\n",
        "file_path = \"train.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "print(\"Shape:\", df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCZoOS-wpYkL"
      },
      "source": [
        "2. Let's look at the data and get started with some preprocessing. Some of the columns, e.g <span style=\"font-weight: 500\"> *Name*</span>, simply identify a person and are not useful for prediction tasks. Try to identify these columns, and remove them.\n",
        "\n",
        "    <span style=\"font-weight: 500\"> *Hint: The shape of the data should now be (891, 9).*</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m18V-jpTpYkL"
      },
      "outputs": [],
      "source": [
        "# Use this cell for your code\n",
        "df = df.drop(columns=['Name', 'Sex', 'Age'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48AcPDOrpYkL"
      },
      "source": [
        "3. The column <span style=\"font-weight: 500\">*Cabin*</span> contains a letter and a number. A smart catch at this point would be to notice that the letter stands for the deck level on the ship. Keeping just the deck information would be more informative when developing, e.g. a classifier that predicts whether a passenger survived. The next step in our preprocessing will be to add a new column to the dataset, which consists simply of the deck letter. You can then remove the original <span style=\"font-weight: 500\">*Cabin*</span>-column.\n",
        "\n",
        "<span style=\"font-weight: 500\">*Hint: The deck letters should be ['A' 'B' 'C' 'D' 'E' 'F' 'G' 'T'].*</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQX5LmWrpYkL"
      },
      "outputs": [],
      "source": [
        "# Use this cell for your code\n",
        "df[\"Level\"] = df[\"Cabin\"].str[0]\n",
        "df = df.drop(columns=['Name', 'Sex', 'Age','Cabin'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mupSH5tXpYkL"
      },
      "source": [
        "4. You’ll notice that some of the columns, such as the previously added deck number, are [categorical](https://en.wikipedia.org/wiki/Categorical_variable). To preprocess the categorical variables so that they're ready for further computation, we need to avoid the current string format of the values. This means the next step for each categorical variable is to transform the string values to numeric ones, that correspond to a unique integer ID representative of each distinct category. This process is called label encoding and you can read more about it [here](https://pandas.pydata.org/docs/user_guide/categorical.html).\n",
        "\n",
        "    <span style=\"font-weight: 500\">*Hint: Pandas can do this for you.*</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfb0_uzRpYkL"
      },
      "outputs": [],
      "source": [
        "# Use this cell for your code\n",
        "for col in df.select_dtypes(include=[\"object\"]).columns:\n",
        "    df[col] = df[col].astype(\"category\").cat.codes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32_hHxGLpYkL"
      },
      "source": [
        "5. Next, let's look into missing value **imputation**. Some of the rows in the data have missing values, e.g when the cabin number of a person is unknown. Most machine learning algorithms have trouble with missing values, and they need to be handled during preprocessing:\n",
        "\n",
        "    a) For continuous variables, replace the missing values with the mean of the non-missing values of that column.\n",
        "\n",
        "    b) For categorical variables, replace the missing values with the mode of the column.\n",
        "\n",
        "    <span style=\"font-weight: 500\">*Remember: Even though in the previous step we transformed categorical variables into their numeric representation, they are still categorical.*</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0kh2bbGpYkL"
      },
      "outputs": [],
      "source": [
        "# Use this cell for your code\n",
        "for col in df.columns:\n",
        "    if df[col].dtype in [\"float64\", \"int64\"]:\n",
        "          mean_value = df[col].mean()\n",
        "          df[col] = df[col].fillna(mean_value)\n",
        "    else: \n",
        "        mode_value = df[col].mode()[0]\n",
        "        df[col] = df[col].fillna(mode_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15Kbmgx9pYkM"
      },
      "source": [
        "6. At this point, all data is numeric. Write the data, with the modifications we made, to a  <span style=\"font-weight: 500\"> .csv</span> file. Then, write another file, this time in <span style=\"font-weight: 500\">JSON</span> format, with the following structure:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "J_EhC78NpYkM"
      },
      "outputs": [],
      "source": [
        "#[\n",
        "#    {\n",
        "#        \"Deck\": 0,\n",
        "#        \"Age\": 20,\n",
        "#        \"Survived\", 0\n",
        "#        ...\n",
        "#    },\n",
        "#    {\n",
        "#        ...\n",
        "#    }\n",
        "#]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxF-ehbapYkM"
      },
      "outputs": [],
      "source": [
        "# Use this cell for your code\n",
        "csv_output = \"output.csv\"\n",
        "df.to_csv(csv_output, index=False)\n",
        "json_output = \"output.json\"\n",
        "df.to_json(json_output, orient=\"records\", indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnkAgzHjpYkM"
      },
      "source": [
        "Study the records and try to see if there is any evident pattern in terms of chances of survival."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddqs0UqLpYkM"
      },
      "source": [
        "**Remember to submit your code on the MOOC platform. You can return this Jupyter notebook (.ipynb) or .py, .R, etc depending on your programming preferences.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGnzPePKpYkM"
      },
      "source": [
        "## Exercise 2 | Titanic 2.0: exploratory data analysis\n",
        "\n",
        "In this exercise, we’ll continue to study the Titanic dataset from the last exercise. Now that we have done some preprocessing, it’s time to look at the data with some exploratory data analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5nTB9ExpYkM"
      },
      "source": [
        "1. First investigate each feature variable in turn. For each categorical variable, find out the mode, i.e., the most frequent value. For numerical variables, calculate the median value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKdMpHZApYkM"
      },
      "outputs": [],
      "source": [
        "# Use this cell for your code\n",
        "for col in df.columns:\n",
        "    if df[col].dtype == \"float64\" or df[col].dtype == \"int64\":\n",
        "        median_value = df[col].median()\n",
        "        print(\"Median:\", median_value)\n",
        "    else:\n",
        "        mode_value = df[col].mode()[0]\n",
        "        print(\"Mode:\", mode_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOjLqfDkpYkM"
      },
      "source": [
        "2. Next, combine the modes of the categorical variables, and the medians of the numerical variables, to construct an imaginary “average survivor”. This \"average survivor\" should represent the typical passenger of the class of passengers who survived. Also following the same principle, construct the “average non-survivor”.\n",
        "\n",
        "    <span style=\"font-weight: 500\">*Hint 1: What are the average/most frequent variable values for a non-survivor?*</span>\n",
        "    \n",
        "    <span style=\"font-weight: 500\">*Hint 2: You can split the dataframe in two: one subset containing all the survivors and one consisting of all the non-survivor instances. Then, you can use the summary statistics of each of these dataframe to create a prototype \"average survivor\" and \"average non-survivor\", respectively.*</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhUU2GIDpYkM"
      },
      "outputs": [],
      "source": [
        "# Use this cell for your code\n",
        "sur = df[df[\"Survived\"] == 1]\n",
        "non_sur = df[df[\"Survived\"] == 0]\n",
        "\n",
        "def average_passenger(subset):\n",
        "    avg_passenger = {}\n",
        "    for col in subset.columns:\n",
        "        if col == \"Survived\":\n",
        "            avg_passenger[col] = subset[col].mode()[0]\n",
        "        elif col in [\"Embarked\", \"Level\"]:\n",
        "            avg_passenger[col] = subset[col].mode()[0]\n",
        "        else: \n",
        "            avg_passenger[col] = subset[col].median()\n",
        "    return avg_passenger\n",
        "\n",
        "average_sur = average_passenger(sur)\n",
        "average_non_sur = average_passenger(non_sur)\n",
        "\n",
        "print(\"Average survivor:\", average_sur)\n",
        "print(\"Average non survivor:\", average_non_sur)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3Bax_hlpYkM"
      },
      "source": [
        "3. Next, let's study the distributions of the variables in the two groups (survivor/non-survivor). How well do the average cases represent the respective groups? Can you find actual passengers that are very similar to the (average) representative of their own group? Can you find passengers that are very similar to the (average) representative of the other group?\n",
        "\n",
        "    <span style=\"font-weight: 500\">*Note: Feel free to choose EDA methods according to your preference: non-graphical/graphical, static/interactive - anything goes.*</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "mNsSMXRMpYkM"
      },
      "outputs": [],
      "source": [
        "# Use this cell for your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cc96MC9CpYkM"
      },
      "source": [
        "4. Next, let's continue the analysis by looking into pairwise and multivariate relationships between the variables in the two groups. Try to visualize two variables at a time using, e.g., scatter plots and use a different color to encode the survival status.\n",
        "\n",
        "    <span style=\"font-weight: 500\">*Hint 1: You can also check out Seaborn's pairplot function, if you wish.*</span>\n",
        "\n",
        "    <span style=\"font-weight: 500\">*Hint 2: To better show many data points with the same value for a given variable, you can use either transparency or ‘jitter’.*</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "fexBqTMQpYkM"
      },
      "outputs": [],
      "source": [
        "# Use this cell for your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNOrCjKzpYkM"
      },
      "source": [
        "5. Finally, recall the preprocessing we did in the first exercise. What can you say about the effect of the choices that were made to use the mode and mean to impute missing values, instead of, for example, ignoring passengers with missing data?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8TeRhWapYkM"
      },
      "source": [
        "*Use this (markdown) cell for your written answer*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsrhB5wvpYkM"
      },
      "source": [
        "**Remember to submit your code on the MOOC platform. You can return this Jupyter notebook (.ipynb) or .py, .R, etc depending on your programming preferences.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqgXjyklpYkM"
      },
      "source": [
        "## Exercise 3 | Working with text data 2.0\n",
        "\n",
        "This exercise is related to the second exercise from last week. Find the saved <span style=\"font-weight: 500\">pos.txt</span> and <span style=\"font-weight: 500\">neg.txt</span> files, or, alternatively, you can find the week 1 example solutions on the MOOC platform after Tuesday."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-DfVlSWpYkM"
      },
      "source": [
        "1. Find the most common words in each file (positive and negative). Examine the results. Do they tend to be general terms relating to the nature of the data? How well do they indicate positive/negative sentiment?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDsK6jXMpYkM"
      },
      "outputs": [],
      "source": [
        "# Use this cell for your code\n",
        "pos_path = \"pos.txt\"\n",
        "neg_path = \"neg.txt\"\n",
        "\n",
        "with open(pos_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    words_pos = f.read().split()\n",
        "    \n",
        "with open(neg_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    words_neg = f.read().split()\n",
        "    \n",
        "def word_counter(words):\n",
        "    freq = {}\n",
        "    for w in words:\n",
        "        freq[w] = freq.get(w,0) + 1\n",
        "    return freq\n",
        "\n",
        "freq_pos = word_counter(words_pos)\n",
        "freq_neg = word_counter(words_neg)\n",
        "\n",
        "common_words = set(freq_pos.keys()) & set(freq_neg.keys())\n",
        "\n",
        "def most_common(freq_dict):\n",
        "    most_common_word = None\n",
        "    max_count = 0\n",
        "    \n",
        "    for w, count in freq_dict.items():\n",
        "        if count > max_count:\n",
        "            most_common_word = w\n",
        "            max_count = count\n",
        "    return most_common_word, max_count\n",
        "\n",
        "word_pos, count_pos = most_common(freq_pos)\n",
        "word_neg, count_neg = most_common(freq_neg)\n",
        "\n",
        "print(\"Most common word in pos.txt:\", word_pos,\",and appears:\", count_pos,\"times.\")\n",
        "print(\"Most common word in neg.txt:\", word_neg,\",and appears:\", count_neg,\"times.\")\n",
        "\n",
        "The most common word in the positive review document is 'great' and in the negative review document it is 'work'. \n",
        "You could state that great is positive and the work could refer to a more negative review, but you can't really\n",
        "draw a conclusion from this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlU_trsrpYkM"
      },
      "source": [
        "2. Compute a [TF/IDF](https://en.wikipedia.org/wiki/Tf–idf) vector for each of the two text files, and make them into a <span style=\"font-weight: 500\">2 x m</span> matrix, where <span style=\"font-weight: 500\">m</span> is the number of unique words in the data. The problem with using the most common words in a review to analyze its contents is that words that are common overall will be common in all reviews (both positive and negative). This means that they probably are not good indicators about the sentiment of a specific review. TF/IDF stands for Term Frequency / Inverse Document Frequency (here the reviews are the documents), and is designed to help by taking into consideration not just the number of times a term occurs (term frequency), but also how many times a word exists in other reviews as well (inverse document frequency). You can use any variant of the formula, as well as off-the-shelf implementations. <span style=\"font-weight: 500\">*Hint: You can use [sklearn](http://scikit-learn.org/).*</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tt_t-Lx8pYkM"
      },
      "outputs": [],
      "source": [
        "# Use this cell for your code\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "pos_path = \"pos.txt\"\n",
        "neg_path = \"neg.txt\"\n",
        "\n",
        "with open(pos_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    text_pos = f.read()\n",
        "    \n",
        "with open(neg_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    text_neg = f.read()\n",
        "    \n",
        "documents = [text_pos, text_neg]\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "tfidf_matrix_dense = tfidf_matrix.toarray()\n",
        "words = vectorizer.get_feature_names_out()\n",
        "\n",
        "print(\"Shape:\", tfidf_matrix_dense.shape)\n",
        "print(\"Words:\", words)\n",
        "print(tfidf_matrix_dense)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lsOuLeOpYkM"
      },
      "source": [
        "3. List the words with the highest TF/IDF score in each class (positive | negative), and compare them to the most common words. What do you notice? Did TF/IDF work as expected?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1tkcbH5pYkM"
      },
      "outputs": [],
      "source": [
        "# Use this cell for your codefrom sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "pos_path = \"pos.txt\"\n",
        "neg_path = \"neg.txt\"\n",
        "\n",
        "with open(pos_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    text_pos = f.read()\n",
        "    \n",
        "with open(neg_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    text_neg = f.read()\n",
        "    \n",
        "documents = [text_pos, text_neg]\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "words = vectorizer.get_feature_names_out()\n",
        "tfidf_dense = tfidf_matrix.toarray()\n",
        "\n",
        "df_tfidf = pd.DataFrame(tfidf_dense, index=[\"pos.txt\", \"neg.txt\"], columns=words)\n",
        "\n",
        "def top_tfidf_words(row, n=10):\n",
        "    sorted_idx = np.argsort(-row.values)[:n]\n",
        "    return [(row.index[i], row.values[i]) for i in sorted_idx]\n",
        "\n",
        "top_pos = top_tfidf_words(df_tfidf.loc[\"pos.txt\"], n=10)\n",
        "top_neg = top_tfidf_words(df_tfidf.loc[\"neg.txt\"], n=10)\n",
        "\n",
        "print(\"Top positive words:\")\n",
        "for word, score in top_pos:\n",
        "    print(word,\":\", score)\n",
        "    \n",
        "print(\"Top negative words:\")\n",
        "for word, score in top_neg:\n",
        "    print(word,\":\", score)\n",
        "\n",
        "You get the same results as with the previous question, so that is no surprise. With this output I managed to print\n",
        "a list of the most common words and you can see that the second most common word in pos.txt is work, which is the\n",
        "most common word in neg.txt. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9z_vWhnXpYkM"
      },
      "source": [
        "4. Plot the words in each class with their corresponding TF/IDF scores. Note that there will be a lot of words, so you’ll have to think carefully to make your chart clear! If you can’t plot them all, plot a subset – think about how you should choose this subset.\n",
        "\n",
        "    <span style=\"font-weight: 500\">*Hint: you can use word clouds. But feel free to challenge yourselves to think of any other meaningful way to visualize this information!*</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eUJ3HxlpYkN"
      },
      "outputs": [],
      "source": [
        "# Use this cell for your code\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pos_path = \"pos.txt\"\n",
        "neg_path = \"neg.txt\"\n",
        "\n",
        "with open(pos_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    text_pos = f.read()\n",
        "    \n",
        "with open(neg_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    text_neg = f.read()\n",
        "    \n",
        "documents = [text_pos, text_neg]\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "words = vectorizer.get_feature_names_out()\n",
        "tfidf_dense = tfidf_matrix.toarray()\n",
        "\n",
        "df_tfidf = pd.DataFrame(tfidf_dense, index=[\"pos.txt\", \"neg.txt\"], columns=words)\n",
        "\n",
        "def top_tfidf_words(row, n=20):\n",
        "    sorted_idx = np.argsort(-row.values)[:n]\n",
        "    return [(row.index[i], row.values[i]) for i in sorted_idx]\n",
        "\n",
        "top_pos = top_tfidf_words(df_tfidf.loc[\"pos.txt\"], n=20)\n",
        "top_neg = top_tfidf_words(df_tfidf.loc[\"neg.txt\"], n=20)\n",
        "\n",
        "print(\"Top positive words:\")\n",
        "for word, score in top_pos:\n",
        "    print(word,\":\", score)\n",
        "    \n",
        "print(\"Top negative words:\")\n",
        "for word, score in top_neg:\n",
        "    print(word,\":\", score)\n",
        "    \n",
        "def plot_top_words(top_list, title, ax):\n",
        "    words = [w for w, _ in top_list][::-1]\n",
        "    scores = [s for _, s in top_list][::-1]\n",
        "    y_pos = np.arange(len(words))\n",
        "    ax.barh(y_pos, scores, color=\"skyblue\", edgecolor=\"k\")\n",
        "    ax.set_yticks(y_pos)\n",
        "    ax.set_xlabel(\"TF-IDF score\")\n",
        "    ax.set_title(title)\n",
        "    ax.invert_yaxis()\n",
        "    \n",
        "fig, axes = plt.subplots(1,2)\n",
        "plot_top_words(top_pos, \"Top TF-IDF words - pos.txt\", axes[0])\n",
        "plot_top_words(top_neg, \"Top TF-IDF words - neg.txt\", axes[1])\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8-yzV5HpYkN"
      },
      "source": [
        "**Remember to submit your code on the MOOC platform. You can return this Jupyter notebook (.ipynb) or .py, .R, etc depending on your programming preferences.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_Xd1n98pYkQ"
      },
      "source": [
        "## Exercise 4 | Junk charts\n",
        "\n",
        "There’s a thriving community of chart enthusiasts who keep looking for statistical graphics that they find inappropriate, and which they call “junk charts”, and who often also propose ways to improve them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w78AogNWpYkQ"
      },
      "source": [
        "1. Find at least three statistical visualizations you think are not very good and identify their problems. Copying examples from various junk chart websites is not accepted – you should find your own junk charts, out in the wild. You should be able to find good (or rather, bad) examples quite easily since a significant fraction of charts can have at least *some* issues. The examples you choose should also have different problems, e.g., try to avoid collecting three bar charts, all with problematic axes. Instead, try to find as interesting and diverse examples as you can."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJfbaQH2pYkQ"
      },
      "source": [
        "2. Try to produce improved versions of the charts you selected. The data is of course often not available, but perhaps you can try to extract it, at least approximately, from the chart. Or perhaps you can simulate data that looks similar enough to make the point.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-rwa7IrpYkQ"
      },
      "source": [
        "**Submit a PDF with all the charts (the ones you found and the ones you produced).**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
